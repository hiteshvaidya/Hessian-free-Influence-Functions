{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bf1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from loguru import logger\n",
    "import time\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle as pkl\n",
    "from safetensors.torch import save_file, load_file\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf695c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set NLTK data path to project's data directory\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'data', 'nltk_data')\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Download required NLTK data\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    print(f\"Downloading NLTK data to: {nltk_data_path}\")\n",
    "    nltk.download('treebank', download_dir=nltk_data_path)\n",
    "    nltk.download('punkt', download_dir=nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81bc3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess text data from Penn Treebank corpus.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Contains three lists of tokenized sentences:\n",
    "            - train_data: Training set (80% of data)\n",
    "            - test_data: Test set (10% of data) \n",
    "            - val_data: Validation set (10% of data)\n",
    "            \n",
    "    Each sentence is preprocessed by:\n",
    "        1. Converting to lowercase\n",
    "        2. Tokenizing using TreebankWordTokenizer\n",
    "        3. Splitting into train/test/val sets\n",
    "    \"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    \n",
    "    # Get sentences from Penn Treebank corpus\n",
    "    sentences = treebank.sents()\n",
    "\n",
    "    # Process each sentence\n",
    "    processed = []\n",
    "    for sent in sentences:\n",
    "        # E sent is a list of words\n",
    "        # Join the sentence into a single string and tokenize\n",
    "        text = ' '.join(sent)\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        processed.append(tokens)\n",
    "\n",
    "    # Split into train, test, and validation sets\n",
    "    train_data = processed[:int(len(processed) * 0.8)]\n",
    "    test_data = processed[int(len(processed) * 0.8):int(len(processed) * 0.9)]\n",
    "    val_data = processed[int(len(processed) * 0.9):]\n",
    "\n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fa50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voab(data, min_freq=2):\n",
    "    \"\"\"\n",
    "    Build vocabulary from training data with minimum frequency threshold.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of tokenized sentences where each sentence is a list of tokens\n",
    "        min_freq (int, optional): Minimum frequency threshold for including words. Defaults to 2.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Contains two dictionaries:\n",
    "            - word_to_idx: Maps words to unique integer indices\n",
    "            - idx_to_word: Maps indices back to words\n",
    "            \n",
    "    The vocabulary includes special tokens:\n",
    "        - <unk>: Unknown words\n",
    "        - <pad>: Padding token\n",
    "        - <bos>: Beginning of sentence\n",
    "        - <eos>: End of sentence\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for sent in data:\n",
    "        counter.update(sent)\n",
    "\n",
    "    # Create vocabulary with special tokens\n",
    "    words = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "    words.extend([word for word, freq in counter.items() if freq >= min_freq])\n",
    "\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
    "    idx_to_word = {idx: word for idx, word in enumerate(words)}\n",
    "\n",
    "    return word_to_idx, idx_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e49a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, word_to_idx):\n",
    "    \"\"\"\n",
    "    Process raw text data into model-ready format by converting tokens to indices.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of tokenized sentences where each sentence is a list of tokens\n",
    "        word_to_idx (dict): Dictionary mapping words to unique integer indices\n",
    "        \n",
    "    Returns:\n",
    "        list: List of torch tensors, where each tensor contains the indices for a sentence\n",
    "            including <bos> and <eos> tokens\n",
    "            \n",
    "    Each sentence is processed by:\n",
    "        1. Converting tokens to their vocabulary indices\n",
    "        2. Adding beginning-of-sentence (<bos>) and end-of-sentence (<eos>) tokens\n",
    "        3. Converting to a PyTorch tensor\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sent in data:\n",
    "        # convert tokens to indices\n",
    "        indices = [word_to_idx.get(token, word_to_idx['<unk>']) for token in sent]\n",
    "        # Add <bos> and <eos> tokens\n",
    "        indices = [word_to_idx['<bos>']] + indices + [word_to_idx['<eos>']]\n",
    "        processed.append(torch.tensor(indices))\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e28228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, word_to_idx, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create batches from processed data for model training.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of torch tensors containing processed sentences\n",
    "        word_to_idx (dict): Dictionary mapping words to unique integer indices\n",
    "        batch_size (int, optional): Size of each batch. Defaults to 32.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of torch tensors, where each tensor is a batch of padded sequences\n",
    "            with shape (batch_size, max_sequence_length)\n",
    "            \n",
    "    The function:\n",
    "        1. Sorts sequences by length in descending order for efficient padding\n",
    "        2. Groups sequences into batches of specified size\n",
    "        3. Pads shorter sequences in each batch to match the longest sequence\n",
    "        4. Converts batches to torch tensors\n",
    "    \"\"\"\n",
    "    data.sort(key=lambda x: len(x), reverse=True)\n",
    "    total_len = len(data)\n",
    "    num_batches = (total_len + batch_size - 1) // batch_size\n",
    "\n",
    "    batches = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        max_len = len(batch[0])\n",
    "        padded = [torch.cat([seq, torch.tensor([word_to_idx['<pad>']] * (max_len - len(seq)))]) if len(seq) < max_len else seq for seq in batch]\n",
    "        batches.append(torch.stack(padded))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3b46a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for language modeling using LSTM.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary\n",
    "        embed_size (int): Dimension of word embeddings\n",
    "        hidden_size (int): Number of features in the hidden state\n",
    "        num_layers (int): Number of recurrent layers\n",
    "        cell (str, optional): Type of RNN cell to use. Currently only supports 'lstm'. Defaults to 'lstm'\n",
    "        dropout (float, optional): Dropout probability. Defaults to 0.5\n",
    "        \n",
    "    Attributes:\n",
    "        layers (int): Number of recurrent layers\n",
    "        hidden_size (int): Size of hidden state\n",
    "        embed (nn.Embedding): Word embedding layer\n",
    "        cell (nn.LSTM): LSTM layer\n",
    "        dropout (nn.Dropout): Dropout layer\n",
    "        fc (nn.Linear): Final linear layer that maps to vocabulary size\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, cell='lstm', dropout=0.5):\n",
    "        super(Network, self).__init__()\n",
    "        self.layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.cell = None\n",
    "        if cell == 'lstm':\n",
    "            # Point: difference between nn.LSTM and nn.LSTMCell\n",
    "            self.cell = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        elif cell == 'gru':\n",
    "            self.cell = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length)\n",
    "            hidden (tuple, optional): Initial hidden state. Defaults to None\n",
    "            \n",
    "        Returns:\n",
    "            tuple:\n",
    "                - logits (torch.Tensor): Output logits of shape (batch_size, sequence_length, vocab_size)\n",
    "                - hidden (tuple): Final hidden state and cell state\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        if hidden is None:\n",
    "            h0 = torch.zeros(self.layers, batch_size, self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.layers, batch_size, self.hidden_size).to(x.device)\n",
    "            hidden = (h0, c0)\n",
    "\n",
    "        embeds = self.dropout(self.embed(x))    #point: why dropout here?\n",
    "        output, hidden = self.cell(embeds, hidden)\n",
    "        output = self.dropout(output)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95be1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_batches, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model\n",
    "        train_batches (torch.Tensor): Training data batches\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer for updating model parameters\n",
    "        \n",
    "    Returns:\n",
    "        float: Average loss over all batches for this epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch[:, :-1].to(device)   # all tokens except last\n",
    "        targets = batch[:, 1:].to(device)  # all tokens except first\n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                         targets.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "    # compute loss\n",
    "    total_loss += loss.item()\n",
    "    total_loss = total_loss / len(train_batches)\n",
    "\n",
    "    return total_loss        \n",
    "\n",
    "\n",
    "def evaluate(model, eval_batches, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation/test data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model\n",
    "        eval_batches (torch.Tensor): Evaluation data batches\n",
    "        criterion: Loss function\n",
    "        \n",
    "    Returns:\n",
    "        float: Average loss over all batches in the evaluation set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # point: why no_grad() is needed when we have model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_batches:\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                             targets.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "    return total_loss / len(eval_batches)\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    return torch.exp(torch.tensor(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5848862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(train_metric, valid_metric, title, xlabel, ylabel, figname):\n",
    "    \"\"\"\n",
    "    Visualize and save a metric plot.\n",
    "    \n",
    "    Args:\n",
    "        metric (list): Values to plot\n",
    "        title (str): Title of the plot\n",
    "        xlabel (str): Label for x-axis\n",
    "        ylabel (str): Label for y-axis \n",
    "        figname (str): Filename to save the plot\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.plot(train_metric, label='train')\n",
    "    plt.plot(valid_metric, label='valid')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.savefig(figname)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b127d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load vocabulary mappings\n",
    "def load_vocab(vocab_path):\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab_dict = json.load(f)\n",
    "    return vocab_dict['word_to_idx'], vocab_dict['idx_to_word']\n",
    "\n",
    "# function to load tensors from safetensors file\n",
    "def load_tensors(file_path):\n",
    "    loaded_tensors = load_file(file_path)\n",
    "    return loaded_tensors['train'], loaded_tensors['valid'], loaded_tensors['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95984d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, word_to_idx, idx_to_word, device, seed_text='the', max_length=20):\n",
    "    \"\"\"Generate text using the trained language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained language model\n",
    "        word_to_idx (dict): Dictionary mapping words to indices\n",
    "        idx_to_word (dict): Dictionary mapping indices to words  \n",
    "        seed_text (str, optional): Initial text to condition generation on. Defaults to 'the'.\n",
    "        max_length (int, optional): Maximum number of words to generate. Defaults to 20.\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated text as a space-separated string of words\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    words = seed_text.lower().split()\n",
    "    indices = [word_to_idx.get(word, word_to_idx['<unk>']) for word in words]\n",
    "    indices = [word_to_idx['<bos>']] + indices\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            input_tensor = torch.tensor(indices).unsqueeze(0).to(device)\n",
    "            output, _ = model(input_tensor)\n",
    "            next_token_idx = output[0, -1].argmax().item()\n",
    "\n",
    "            if next_token_idx == word_to_idx['<eos>']:\n",
    "                break\n",
    "\n",
    "            indices.append(next_token_idx)\n",
    "\n",
    "    generated_words = [idx_to_word[idx] for idx in indices[1:]] # skip <bos>\n",
    "\n",
    "    return ' '.join(generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09f7e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_impact_function(model, valid_batches, criterion, device):\n",
    "    \"\"\"Get the impact function for each parameter in the model.\n",
    "    \n",
    "    The impact function is calculated by accumulating gradients over the validation set.\n",
    "    This represents how much each parameter impacts the model's performance on validation data.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained language model\n",
    "        valid_batches: Validation data batches\n",
    "        criterion: Loss function\n",
    "        device: Device to run computations on (cuda/cpu/mps)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping parameter names to their accumulated gradients\n",
    "    \"\"\"\n",
    "    # initialize gradient dictionary to store gradients of each parameter\n",
    "    valid_grads = {name: torch.zeros_like(param.data) for name, param in model.named_parameters() if param.requires_grad}\n",
    "    \n",
    "    # set model to training mode so the dropout is applied\n",
    "    model.eval()\n",
    "\n",
    "    for batch in valid_batches:\n",
    "        model.zero_grad()\n",
    "        inputs = batch[:, :-1].to(device)\n",
    "        targets = batch[:, 1:].to(device)\n",
    "        \n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                         targets.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                valid_grads[name] += param.grad.detach()\n",
    "\n",
    "    grads = torch.cat([value.view(-1) for name, value in valid_grads.items()])\n",
    "    \n",
    "    return grads\n",
    "    \n",
    "def get_fisher_matrix(model, train_batches, criterion, device, epsilon=1e-7, lambda_reg=1E-3):\n",
    "    model.eval()\n",
    "    fisher_matrix = {name: torch.zeros_like(param.data) for name, param in model.named_parameters() if param.requires_grad}\n",
    "    grad_matrix = {name: torch.zeros_like(param.data) for name, param in model.named_parameters() if param.requires_grad}\n",
    "    n_samples = 0\n",
    "    \n",
    "    for batch in train_batches:\n",
    "        model.zero_grad()\n",
    "        inputs = batch[:, :-1].to(device)\n",
    "        targets = batch[:, 1:].to(device)\n",
    "        n_samples += inputs.size(0)\n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                         targets.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # compute the empirical fisher matrix (diagonal of the real fisher matrix) \n",
    "        # and collect gradients\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                fisher_matrix[name] += torch.pow(param.grad.detach(), 2)\n",
    "        \n",
    "    # normalize the fisher values per parameter and invert\n",
    "    for name in fisher_matrix:\n",
    "        temp = (fisher_matrix[name] / (n_samples)) + lambda_reg * 1.0\n",
    "        fisher_matrix[name] = 1 / (temp + epsilon)\n",
    "\n",
    "    grads = torch.cat([value.view(-1) for name, value in fisher_matrix.items()])\n",
    "    \n",
    "    return grads\n",
    "\n",
    "\n",
    "def get_fisher_influence_function(model, train_batches, valid_batches, criterion, device):\n",
    "    fisher_values = get_fisher_matrix(model, train_batches, criterion, device)\n",
    "    valid_grads = get_impact_function(model, valid_batches, criterion, device)\n",
    "    # valid_grads = torch.unsqueeze(valid_grads, 0)\n",
    "    \n",
    "    influence_values = []\n",
    "\n",
    "    for batch in train_batches:\n",
    "        for b in range(batch.size(0)):\n",
    "            model.zero_grad()\n",
    "            inputs = batch[b: b+1, :-1].to(device)\n",
    "            targets = batch[b: b+1, 1:].to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                         targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            grads = torch.cat([param.grad.detach().view(-1) for name, param in model.named_parameters() if param.requires_grad])\n",
    "\n",
    "            # fisher_values = fisher_values * grads\n",
    "\n",
    "            influence_values.append(torch.sum(valid_grads * fisher_values * grads))\n",
    "\n",
    "    influence_values = torch.tensor(influence_values)\n",
    "    print(f\"influence_values: {influence_values.shape}\")\n",
    "    return influence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86196cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-01 14:23:59.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mUsing device: mps\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:00.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mLoaded data\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:00.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mBuilt vocabulary of size 4899\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:00.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mprocessed all data\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:00.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved vocabulary mappings to JSON file\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:00.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mCreated batches for train, test, valid tensors\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:00.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mSaved processed tensors to file\u001b[0m\n",
      "/var/folders/nk/bx00v2zs29x350cy3n29pbbw0000gn/T/ipykernel_56970/381095448.py:30: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Scalar.cpp:23.)\n",
      "  total_loss += loss.item()\n",
      "\u001b[32m2025-05-01 14:24:05.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 1/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:05.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.06, Perplexity: 1.06\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:05.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 6.25, Perplexity: 517.81\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:05.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 517.81\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:07.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 2/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:07.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.05, Perplexity: 1.06\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:07.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 6.20, Perplexity: 490.87\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:07.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 490.87\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:08.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 3/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:08.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.05, Perplexity: 1.05\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:08.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 5.53, Perplexity: 252.29\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:08.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 252.29\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:10.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 4/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:10.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.05, Perplexity: 1.05\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:10.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 5.32, Perplexity: 205.40\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:10.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 205.40\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:11.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 5/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:11.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.05, Perplexity: 1.05\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:11.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 5.18, Perplexity: 177.57\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:12.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 177.57\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:13.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 6/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:13.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.05, Perplexity: 1.05\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:13.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 5.08, Perplexity: 161.20\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:13.500\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 161.20\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:14.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 7/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:14.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.04, Perplexity: 1.05\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:14.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 5.00, Perplexity: 148.74\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:15.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 148.74\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:16.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 8/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:16.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.04, Perplexity: 1.04\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:16.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.98, Perplexity: 145.97\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:16.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 145.97\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:17.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 9/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:17.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.04, Perplexity: 1.04\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:17.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.94, Perplexity: 139.61\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:18.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 139.61\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:19.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 10/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:19.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.04, Perplexity: 1.04\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:19.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.92, Perplexity: 137.41\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:19.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 137.41\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:20.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 11/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:20.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.04, Perplexity: 1.04\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:20.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.89, Perplexity: 132.39\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:20.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 132.39\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:22.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 12/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:22.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.04, Perplexity: 1.04\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:22.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.89, Perplexity: 133.48\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:23.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 13/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:23.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.04, Perplexity: 1.04\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:23.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.90, Perplexity: 134.11\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:25.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 14/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:25.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.03\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:25.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.89, Perplexity: 132.43\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:27.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 15/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:27.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.04\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:27.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.88, Perplexity: 131.55\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:27.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 131.55\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:28.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 16/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:28.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.03\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:28.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.88, Perplexity: 131.13\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:28.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 131.13\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:30.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 17/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:30.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.03\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:30.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.87, Perplexity: 130.19\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:30.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mNew best validation perplexity: 130.19\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:31.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 18/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:31.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.03\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:31.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.87, Perplexity: 130.56\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:33.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 19/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:33.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.03\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:33.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.87, Perplexity: 130.42\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:34.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 20/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:34.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.03\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:34.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.88, Perplexity: 131.19\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:36.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 21/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:36.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.03\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:36.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.89, Perplexity: 132.72\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:38.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 22/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:38.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.03\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:38.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.90, Perplexity: 134.80\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:39.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 23/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:39.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.03\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:39.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.90, Perplexity: 134.40\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:41.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 24/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:41.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.03\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:41.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.91, Perplexity: 136.26\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:42.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mEpoch 25/25:\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:42.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain loss: 0.03, Perplexity: 1.03\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:42.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid loss: 4.94, Perplexity: 139.84\u001b[0m\n",
      "\u001b[32m2025-05-01 14:24:43.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mTest Perplexity with best model: 119.46\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trial_name = 'lstm-1'\n",
    "\n",
    "if not os.path.exists('logs'): os.makedirs('logs')\n",
    "log_dir = os.path.join('logs', trial_name)\n",
    "if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "\n",
    "# configure the logger\n",
    "logger.add(os.path.join(log_dir, 'logs.log'), format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\", level=\"INFO\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# load and tokenize the data\n",
    "train_data, valid_data, test_data = load_and_preprocess_data()\n",
    "logger.info(f\"Loaded data\")\n",
    "# build a vocabulary\n",
    "word_to_idx, idx_to_word = build_voab(train_data)\n",
    "logger.info(f\"Built vocabulary of size {len(word_to_idx)}\")\n",
    "\n",
    "# convert to tensors and add <bos> and <eos>\n",
    "train_tensors = process_data(train_data, word_to_idx)\n",
    "valid_tensors = process_data(valid_data, word_to_idx)\n",
    "test_tensors = process_data(test_data, word_to_idx)\n",
    "logger.info(\"processed all data\")\n",
    "\n",
    "# create data directory if it doesn't exist\n",
    "if not os.path.exists('data'): os.makedirs('data')\n",
    "\n",
    "# save vocabulary mappings\n",
    "vocab_dict = {\n",
    "    'word_to_idx': {word: int(idx) for word, idx in word_to_idx.items()},  # convert any tensor indices to int\n",
    "    'idx_to_word': {int(idx): word for idx, word in idx_to_word.items()}   # convert any tensor indices to int\n",
    "}\n",
    "\n",
    "# This import should be moved to the first cell\n",
    "vocab_path = os.path.join('data', 'treebank_vocab.json')\n",
    "with open(vocab_path, 'w') as f:\n",
    "    json.dump(vocab_dict, f)\n",
    "\n",
    "logger.info(\"Saved vocabulary mappings to JSON file\")\n",
    "\n",
    "\n",
    "# pad the tokens and create batches\n",
    "batch_size = 32\n",
    "train_batches = create_batches(train_tensors, word_to_idx, batch_size)\n",
    "valid_batches = create_batches(valid_tensors, word_to_idx, batch_size)\n",
    "test_batches = create_batches(test_tensors, word_to_idx, batch_size)\n",
    "logger.info(f\"Created batches for train, test, valid tensors\")\n",
    "\n",
    "# save tensors using torch.save\n",
    "tensors_dict = {\n",
    "    'train': [tensor.to('cpu') for tensor in train_batches],\n",
    "    'valid': [tensor.to('cpu') for tensor in valid_batches],\n",
    "    'test': [tensor.to('cpu') for tensor in test_batches]\n",
    "}\n",
    "\n",
    "torch.save(\n",
    "    tensors_dict,\n",
    "    os.path.join('data', 'treebank_batches_tensors.pt')\n",
    ")\n",
    "logger.info(\"Saved processed tensors to file\")\n",
    "\n",
    "# load tensors\n",
    "# tensors_dict = torch.load(os.path.join('data', 'treebank_batches_tensors.pt'))\n",
    "# train_batches = [tensor.to(device) for tensor in tensors_dict['train']]\n",
    "# valid_batches = [tensor.to(device) for tensor in tensors_dict['valid']] \n",
    "# test_batches = [tensor.to(device) for tensor in tensors_dict['test']]\n",
    "\n",
    "\n",
    "# Initialize the model and training components\n",
    "vocab_size = len(word_to_idx)\n",
    "embed_size = 300\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "\n",
    "model = Network(vocab_size, embed_size, hidden_size, num_layers, cell='lstm', dropout=dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<pad>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "train_ppls = []\n",
    "valid_ppls = []\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_valid_ppl = float('inf')\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_loss = train(model, train_batches, criterion, optimizer, device)\n",
    "    valid_loss = evaluate(model, valid_batches, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_ppl = calculate_perplexity(train_loss)\n",
    "    valid_ppl = calculate_perplexity(valid_loss)\n",
    "    \n",
    "    train_ppls.append(train_ppl)\n",
    "    valid_ppls.append(valid_ppl)\n",
    "\n",
    "    logger.info(f'Epoch {e+1}/{num_epochs}:')\n",
    "    logger.info(f'Train loss: {train_loss:.2f}, Perplexity: {train_ppl:.2f}')\n",
    "    logger.info(f'Valid loss: {valid_loss:.2f}, Perplexity: {valid_ppl:.2f}')\n",
    "\n",
    "    if valid_ppl < best_valid_ppl:\n",
    "        best_valid_ppl = valid_ppl\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'best_model.pt'))\n",
    "        logger.info(f'New best validation perplexity: {valid_ppl:.2f}')\n",
    "\n",
    "visualize(train_ppls, valid_ppls, f'Perplexity for {trial_name}', 'epochs', 'ppl', os.path.join(log_dir, 'perplexity.png'))\n",
    "visualize(train_losses, valid_losses, f'Loss for {trial_name}', 'epochs', 'loss', os.path.join(log_dir, 'losses.png'))\n",
    "\n",
    "# Load the best model for final evaluation\n",
    "model.load_state_dict(torch.load(os.path.join(log_dir, 'best_model.pt')))\n",
    "\n",
    "test_loss = evaluate(model, test_batches, criterion, device)\n",
    "test_ppl = calculate_perplexity(test_loss)\n",
    "logger.info(f\"Test Perplexity with best model: {test_ppl:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b49074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the company said 0 it has redeemed its rights .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, word_to_idx, idx_to_word, device, seed_text=\"the\", max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f98bdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5649959\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_params = 0\n",
    "for param in list(model.parameters()):\n",
    "    n_params += param.numel()\n",
    "print(f\"Number of parameters: {n_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute fisher influence function\n",
    "fisher_influence_values = get_fisher_influence_function(model, train_batches, valid_batches, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8742cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_values = {'fisher_influence_values': fisher_influence_values}\n",
    "save_file(influence_values, os.path.join(log_dir, 'fisher_influence_values.safetensors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f4ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptmetal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
