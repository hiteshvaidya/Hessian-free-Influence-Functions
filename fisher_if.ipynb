{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10bf1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from loguru import logger\n",
    "import time\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle as pkl\n",
    "from safetensors.torch import save_file, load_file\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf695c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data to: /home/hvaidya/documents/Hessian-free-Influence-Functions/data/nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/hvaidya/documents/Hessian-free-Influence-\n",
      "[nltk_data]     Functions/data/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/hvaidya/documents/Hessian-free-Influence-\n",
      "[nltk_data]     Functions/data/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Set NLTK data path to project's data directory\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'data', 'nltk_data')\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Download required NLTK data\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    print(f\"Downloading NLTK data to: {nltk_data_path}\")\n",
    "    nltk.download('treebank', download_dir=nltk_data_path)\n",
    "    nltk.download('punkt', download_dir=nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81bc3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess text data from Penn Treebank corpus.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Contains three lists of tokenized sentences:\n",
    "            - train_data: Training set (80% of data)\n",
    "            - test_data: Test set (10% of data) \n",
    "            - val_data: Validation set (10% of data)\n",
    "            \n",
    "    Each sentence is preprocessed by:\n",
    "        1. Converting to lowercase\n",
    "        2. Tokenizing using TreebankWordTokenizer\n",
    "        3. Splitting into train/test/val sets\n",
    "    \"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    \n",
    "    # Get sentences from Penn Treebank corpus\n",
    "    sentences = treebank.sents()\n",
    "\n",
    "    # Process each sentence\n",
    "    processed = []\n",
    "    for sent in sentences:\n",
    "        # E sent is a list of words\n",
    "        # Join the sentence into a single string and tokenize\n",
    "        text = ' '.join(sent)\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        processed.append(tokens)\n",
    "\n",
    "    # Split into train, test, and validation sets\n",
    "    train_data = processed[:int(len(processed) * 0.8)]\n",
    "    test_data = processed[int(len(processed) * 0.8):int(len(processed) * 0.9)]\n",
    "    val_data = processed[int(len(processed) * 0.9):]\n",
    "\n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81fa50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voab(data, min_freq=2):\n",
    "    \"\"\"\n",
    "    Build vocabulary from training data with minimum frequency threshold.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of tokenized sentences where each sentence is a list of tokens\n",
    "        min_freq (int, optional): Minimum frequency threshold for including words. Defaults to 2.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Contains two dictionaries:\n",
    "            - word_to_idx: Maps words to unique integer indices\n",
    "            - idx_to_word: Maps indices back to words\n",
    "            \n",
    "    The vocabulary includes special tokens:\n",
    "        - <unk>: Unknown words\n",
    "        - <pad>: Padding token\n",
    "        - <bos>: Beginning of sentence\n",
    "        - <eos>: End of sentence\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for sent in data:\n",
    "        counter.update(sent)\n",
    "\n",
    "    # Create vocabulary with special tokens\n",
    "    words = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "    words.extend([word for word, freq in counter.items() if freq >= min_freq])\n",
    "\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
    "    idx_to_word = {idx: word for idx, word in enumerate(words)}\n",
    "\n",
    "    return word_to_idx, idx_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e49a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, word_to_idx):\n",
    "    \"\"\"\n",
    "    Process raw text data into model-ready format by converting tokens to indices.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of tokenized sentences where each sentence is a list of tokens\n",
    "        word_to_idx (dict): Dictionary mapping words to unique integer indices\n",
    "        \n",
    "    Returns:\n",
    "        list: List of torch tensors, where each tensor contains the indices for a sentence\n",
    "            including <bos> and <eos> tokens\n",
    "            \n",
    "    Each sentence is processed by:\n",
    "        1. Converting tokens to their vocabulary indices\n",
    "        2. Adding beginning-of-sentence (<bos>) and end-of-sentence (<eos>) tokens\n",
    "        3. Converting to a PyTorch tensor\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sent in data:\n",
    "        # convert tokens to indices\n",
    "        indices = [word_to_idx.get(token, word_to_idx['<unk>']) for token in sent]\n",
    "        # Add <bos> and <eos> tokens\n",
    "        indices = [word_to_idx['<bos>']] + indices + [word_to_idx['<eos>']]\n",
    "        processed.append(torch.tensor(indices))\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e28228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, word_to_idx, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create batches from processed data for model training.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of torch tensors containing processed sentences\n",
    "        word_to_idx (dict): Dictionary mapping words to unique integer indices\n",
    "        batch_size (int, optional): Size of each batch. Defaults to 32.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of torch tensors, where each tensor is a batch of padded sequences\n",
    "            with shape (batch_size, max_sequence_length)\n",
    "            \n",
    "    The function:\n",
    "        1. Sorts sequences by length in descending order for efficient padding\n",
    "        2. Groups sequences into batches of specified size\n",
    "        3. Pads shorter sequences in each batch to match the longest sequence\n",
    "        4. Converts batches to torch tensors\n",
    "    \"\"\"\n",
    "    data.sort(key=lambda x: len(x), reverse=True)\n",
    "    total_len = len(data)\n",
    "    num_batches = (total_len + batch_size - 1) // batch_size\n",
    "\n",
    "    batches = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        max_len = len(batch[0])\n",
    "        padded = [torch.cat([seq, torch.tensor([word_to_idx['<pad>']] * (max_len - len(seq)))]) if len(seq) < max_len else seq for seq in batch]\n",
    "        batches.append(torch.stack(padded))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b46a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for language modeling using LSTM.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary\n",
    "        embed_size (int): Dimension of word embeddings\n",
    "        hidden_size (int): Number of features in the hidden state\n",
    "        num_layers (int): Number of recurrent layers\n",
    "        cell (str, optional): Type of RNN cell to use. Currently only supports 'lstm'. Defaults to 'lstm'\n",
    "        dropout (float, optional): Dropout probability. Defaults to 0.5\n",
    "        \n",
    "    Attributes:\n",
    "        layers (int): Number of recurrent layers\n",
    "        hidden_size (int): Size of hidden state\n",
    "        embed (nn.Embedding): Word embedding layer\n",
    "        cell (nn.LSTM): LSTM layer\n",
    "        dropout (nn.Dropout): Dropout layer\n",
    "        fc (nn.Linear): Final linear layer that maps to vocabulary size\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, cell='lstm', dropout=0.5):\n",
    "        super(Network, self).__init__()\n",
    "        self.layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.cell = None\n",
    "        if cell == 'lstm':\n",
    "            # Point: difference between nn.LSTM and nn.LSTMCell\n",
    "            self.cell = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        elif cell == 'gru':\n",
    "            self.cell = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length)\n",
    "            hidden (tuple, optional): Initial hidden state. Defaults to None\n",
    "            \n",
    "        Returns:\n",
    "            tuple:\n",
    "                - logits (torch.Tensor): Output logits of shape (batch_size, sequence_length, vocab_size)\n",
    "                - hidden (tuple): Final hidden state and cell state\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        if hidden is None and isinstance(self.cell, nn.LSTM):\n",
    "            h0 = torch.zeros(self.layers, batch_size, self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.layers, batch_size, self.hidden_size).to(x.device)\n",
    "            hidden = (h0, c0)\n",
    "        elif hidden is None and isinstance(self.cell, nn.GRU):\n",
    "            h0 = torch.zeros(self.layers, batch_size, self.hidden_size).to(x.device)\n",
    "            hidden = h0\n",
    "\n",
    "        embeds = self.dropout(self.embed(x))    #point: why dropout here?\n",
    "        output, hidden = self.cell(embeds, hidden)output = self.dropout(output)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95be1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_batches, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model\n",
    "        train_batches (torch.Tensor): Training data batches\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer for updating model parameters\n",
    "        \n",
    "    Returns:\n",
    "        float: Average loss over all batches for this epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch[:, :-1].to(device)   # all tokens except last\n",
    "        targets = batch[:, 1:].to(device)  # all tokens except first\n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                         targets.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "    # compute loss\n",
    "    total_loss += loss.item()\n",
    "    total_loss = total_loss / len(train_batches)\n",
    "\n",
    "    return total_loss        \n",
    "\n",
    "\n",
    "def evaluate(model, eval_batches, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation/test data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model\n",
    "        eval_batches (torch.Tensor): Evaluation data batches\n",
    "        criterion: Loss function\n",
    "        \n",
    "    Returns:\n",
    "        float: Average loss over all batches in the evaluation set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # point: why no_grad() is needed when we have model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_batches:\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                             targets.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "    return total_loss / len(eval_batches)\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    return torch.exp(torch.tensor(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5848862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(train_metric, valid_metric, title, xlabel, ylabel, figname):\n",
    "    \"\"\"\n",
    "    Visualize and save a metric plot.\n",
    "    \n",
    "    Args:\n",
    "        metric (list): Values to plot\n",
    "        title (str): Title of the plot\n",
    "        xlabel (str): Label for x-axis\n",
    "        ylabel (str): Label for y-axis \n",
    "        figname (str): Filename to save the plot\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.plot(train_metric, label='train')\n",
    "    plt.plot(valid_metric, label='valid')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.savefig(figname)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b127d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load vocabulary mappings\n",
    "def load_vocab(vocab_path):\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab_dict = json.load(f)\n",
    "    return vocab_dict['word_to_idx'], vocab_dict['idx_to_word']\n",
    "\n",
    "# function to load tensors from safetensors file\n",
    "def load_tensors(file_path):\n",
    "    loaded_tensors = load_file(file_path)\n",
    "    return loaded_tensors['train'], loaded_tensors['valid'], loaded_tensors['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95984d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, word_to_idx, idx_to_word, device, seed_text='the', max_length=20):\n",
    "    \"\"\"Generate text using the trained language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained language model\n",
    "        word_to_idx (dict): Dictionary mapping words to indices\n",
    "        idx_to_word (dict): Dictionary mapping indices to words  \n",
    "        seed_text (str, optional): Initial text to condition generation on. Defaults to 'the'.\n",
    "        max_length (int, optional): Maximum number of words to generate. Defaults to 20.\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated text as a space-separated string of words\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    words = seed_text.lower().split()\n",
    "    print(f\"seed words: {words}\")\n",
    "    indices = [word_to_idx.get(word, word_to_idx['<unk>']) for word in words]\n",
    "    indices = [word_to_idx['<bos>']] + indices\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while len(indices) < max_length:\n",
    "            input_tensor = torch.tensor(indices).unsqueeze(0).to(device)\n",
    "            output, _ = model(input_tensor)\n",
    "            next_token_idx = output[0, -1].argmax().item()\n",
    "\n",
    "            if next_token_idx == word_to_idx['<eos>']:\n",
    "                break\n",
    "\n",
    "            indices.append(next_token_idx)\n",
    "\n",
    "    generated_words = [idx_to_word[idx] for idx in indices[1:]] # skip <bos>\n",
    "\n",
    "    return ' '.join(generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09f7e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_impact_function(model, valid_batches, criterion, device):\n",
    "    \"\"\"Get the impact function for each parameter in the model.\n",
    "    \n",
    "    The impact function is calculated by accumulating gradients over the validation set.\n",
    "    This represents how much each parameter impacts the model's performance on validation data.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained language model\n",
    "        valid_batches: Validation data batches\n",
    "        criterion: Loss function\n",
    "        device: Device to run computations on (cuda/cpu/mps)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping parameter names to their accumulated gradients\n",
    "    \"\"\"\n",
    "    # initialize gradient dictionary to store gradients of each parameter\n",
    "    valid_grads = {name: torch.zeros_like(param.data) for name, param in model.named_parameters() if param.requires_grad}\n",
    "    \n",
    "    # set model to training mode so the dropout is applied\n",
    "    model.eval()\n",
    "\n",
    "    for batch in valid_batches:\n",
    "        model.zero_grad()\n",
    "        inputs = batch[:, :-1].to(device)\n",
    "        targets = batch[:, 1:].to(device)\n",
    "        \n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                         targets.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                valid_grads[name] += param.grad.detach()\n",
    "\n",
    "    grads = torch.cat([value.view(-1) for name, value in valid_grads.items()])\n",
    "    \n",
    "    return grads\n",
    "    \n",
    "def get_fisher_matrix(model, train_batches, criterion, device, epsilon=1e-7, lambda_reg=1E-3):\n",
    "    model.eval()\n",
    "    fisher_matrix = {name: torch.zeros_like(param.data) for name, param in model.named_parameters() if param.requires_grad}\n",
    "    grad_matrix = {name: torch.zeros_like(param.data) for name, param in model.named_parameters() if param.requires_grad}\n",
    "    n_samples = 0\n",
    "    \n",
    "    for batch in train_batches:\n",
    "        model.zero_grad()\n",
    "        inputs = batch[:, :-1].to(device)\n",
    "        targets = batch[:, 1:].to(device)\n",
    "        n_samples += inputs.size(0)\n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                         targets.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # compute the empirical fisher matrix (diagonal of the real fisher matrix) \n",
    "        # and collect gradients\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                fisher_matrix[name] += torch.pow(param.grad.detach(), 2)\n",
    "        \n",
    "    # normalize the fisher values per parameter and invert\n",
    "    for name in fisher_matrix:\n",
    "        temp = (fisher_matrix[name] / (n_samples)) + lambda_reg * 1.0\n",
    "        fisher_matrix[name] = 1 / (temp + epsilon)\n",
    "\n",
    "    grads = torch.cat([value.view(-1) for name, value in fisher_matrix.items()])\n",
    "    \n",
    "    return grads\n",
    "\n",
    "\n",
    "def get_fisher_influence_function(model, train_batches, valid_batches, criterion, device):\n",
    "    fisher_values = get_fisher_matrix(model, train_batches, criterion, device)\n",
    "    valid_grads = get_impact_function(model, valid_batches, criterion, device)\n",
    "    # valid_grads = torch.unsqueeze(valid_grads, 0)\n",
    "    \n",
    "    influence_values = []\n",
    "\n",
    "    for batch in train_batches:\n",
    "        for b in range(batch.size(0)):\n",
    "            model.zero_grad()\n",
    "            inputs = batch[b: b+1, :-1].to(device)\n",
    "            targets = batch[b: b+1, 1:].to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                         targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            grads = torch.cat([param.grad.detach().view(-1) for name, param in model.named_parameters() if param.requires_grad])\n",
    "\n",
    "            # fisher_values = fisher_values * grads\n",
    "\n",
    "            influence_values.append(torch.sum(valid_grads * fisher_values * grads))\n",
    "\n",
    "    influence_values = torch.tensor(influence_values)\n",
    "    print(f\"influence_values: {influence_values.shape}\")\n",
    "    return influence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86196cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-02 15:58:17.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mUsing device: cuda\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:17.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mLoaded data\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:17.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mBuilt vocabulary of size 4899\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:18.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mprocessed all data\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:18.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved vocabulary mappings to JSON file\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:18.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mCreated batches for train, test, valid tensors\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:18.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mSaved processed tensors to file\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 4899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-02 15:58:19.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 1/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:19.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: 0.22, Perplexity: 1.24\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:19.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: 22.81, Perplexity: 8094751744.00\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:19.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mNew best validation perplexity: 8094751744.00\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:20.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 2/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:20.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: 0.25, Perplexity: 1.28\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:20.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: 25.06, Perplexity: 76333973504.00\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:22.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 3/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:22.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: 0.19, Perplexity: 1.20\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:22.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: 24.38, Perplexity: 38826250240.00\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:23.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 4/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:23.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: 0.11, Perplexity: 1.12\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:23.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: 13.02, Perplexity: 453524.28\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:23.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mNew best validation perplexity: 453524.28\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:24.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 5/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:24.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: 0.12, Perplexity: 1.13\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:24.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: 10.35, Perplexity: 31362.43\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:25.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mNew best validation perplexity: 31362.43\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:26.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 6/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:26.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:26.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:27.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 7/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:27.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:27.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:28.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 8/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:28.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:28.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:30.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 9/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:30.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:30.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:31.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 10/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:31.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:31.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:32.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 11/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:32.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:32.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:33.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 12/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:33.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:33.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:35.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 13/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:35.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:35.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:36.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 14/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:36.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:36.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:37.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 15/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:37.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:37.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:39.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 16/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:39.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:39.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:40.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 17/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:40.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:40.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:41.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 18/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:41.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:41.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:42.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 19/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:42.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:42.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:44.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 20/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:44.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:44.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:45.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 21/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:45.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:45.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:46.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 22/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:46.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:46.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:47.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 23/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:47.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:47.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:49.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 24/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:49.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:49.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:50.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 25/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:50.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:50.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:51.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 26/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:51.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:51.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:53.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 27/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:53.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:53.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:54.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 28/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:54.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:54.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:55.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 29/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:55.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:55.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:56.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 30/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:56.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:56.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:58.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 31/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:58.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:58.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:59.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 32/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:59.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:58:59.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:00.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 33/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:00.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:00.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:01.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 34/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:01.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:01.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:03.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 35/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:03.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:03.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:04.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 36/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:04.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:04.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:05.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 37/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:05.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:05.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:07.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 38/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:07.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:07.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:08.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 39/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:08.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:08.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:09.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 40/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:09.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:09.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:10.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 41/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:10.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:10.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:12.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 42/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:12.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:12.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:13.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 43/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:13.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:13.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:14.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 44/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:14.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:14.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:15.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 45/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:15.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:15.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:17.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 46/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:17.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:17.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:18.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 47/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:18.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:18.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:19.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 48/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:19.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:19.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:21.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 49/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:21.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:21.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:22.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mEpoch 50/50:\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:22.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:22.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValid loss: nan, Perplexity: nan\u001b[0m\n",
      "\u001b[32m2025-05-02 15:59:22.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mTest Perplexity with best model: 31808.29\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trial_name = 'lstm-1'\n",
    "\n",
    "if not os.path.exists('logs'): os.makedirs('logs')\n",
    "log_dir = os.path.join('logs', trial_name)\n",
    "if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "\n",
    "# configure the logger\n",
    "logger.add(os.path.join(log_dir, 'logs.log'), format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\", level=\"INFO\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# load and tokenize the data\n",
    "train_data, valid_data, test_data = load_and_preprocess_data()\n",
    "logger.info(f\"Loaded data\")\n",
    "# build a vocabulary\n",
    "word_to_idx, idx_to_word = build_voab(train_data)\n",
    "logger.info(f\"Built vocabulary of size {len(word_to_idx)}\")\n",
    "\n",
    "# convert to tensors and add <bos> and <eos>\n",
    "train_tensors = process_data(train_data, word_to_idx)\n",
    "valid_tensors = process_data(valid_data, word_to_idx)\n",
    "test_tensors = process_data(test_data, word_to_idx)\n",
    "logger.info(\"processed all data\")\n",
    "\n",
    "# create data directory if it doesn't exist\n",
    "if not os.path.exists('data'): os.makedirs('data')\n",
    "\n",
    "# save vocabulary mappings\n",
    "vocab_dict = {\n",
    "    'word_to_idx': {word: int(idx) for word, idx in word_to_idx.items()},  # convert any tensor indices to int\n",
    "    'idx_to_word': {int(idx): word for idx, word in idx_to_word.items()}   # convert any tensor indices to int\n",
    "}\n",
    "\n",
    "# This import should be moved to the first cell\n",
    "vocab_path = os.path.join('data', 'treebank_vocab.json')\n",
    "with open(vocab_path, 'w') as f:\n",
    "    json.dump(vocab_dict, f)\n",
    "\n",
    "logger.info(\"Saved vocabulary mappings to JSON file\")\n",
    "\n",
    "\n",
    "# pad the tokens and create batches\n",
    "batch_size = 32\n",
    "train_batches = create_batches(train_tensors, word_to_idx, batch_size)\n",
    "valid_batches = create_batches(valid_tensors, word_to_idx, batch_size)\n",
    "test_batches = create_batches(test_tensors, word_to_idx, batch_size)\n",
    "logger.info(f\"Created batches for train, test, valid tensors\")\n",
    "\n",
    "# save tensors using torch.save\n",
    "tensors_dict = {\n",
    "    'train': [tensor.to('cpu') for tensor in train_batches],\n",
    "    'valid': [tensor.to('cpu') for tensor in valid_batches],\n",
    "    'test': [tensor.to('cpu') for tensor in test_batches]\n",
    "}\n",
    "\n",
    "torch.save(\n",
    "    tensors_dict,\n",
    "    os.path.join('data', 'treebank_batches_tensors.pt')\n",
    ")\n",
    "logger.info(\"Saved processed tensors to file\")\n",
    "\n",
    "# load tensors\n",
    "# tensors_dict = torch.load(os.path.join('data', 'treebank_batches_tensors.pt'))\n",
    "# train_batches = [tensor.to(device) for tensor in tensors_dict['train']]\n",
    "# valid_batches = [tensor.to(device) for tensor in tensors_dict['valid']] \n",
    "# test_batches = [tensor.to(device) for tensor in tensors_dict['test']]\n",
    "\n",
    "\n",
    "# Initialize the model and training components\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "\n",
    "# Instantiate the model\n",
    "model = Network(vocab_size, embed_size, hidden_size, num_layers, cell='lstm', dropout=dropout).to(device)\n",
    "# Instantiate the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<pad>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# Instantiate the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "train_ppls = []\n",
    "valid_ppls = []\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_valid_ppl = float('inf')\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_loss = train(model, train_batches, criterion, optimizer, device)\n",
    "    \n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    valid_loss = evaluate(model, valid_batches, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_ppl = calculate_perplexity(train_loss)\n",
    "    valid_ppl = calculate_perplexity(valid_loss)\n",
    "    \n",
    "    train_ppls.append(train_ppl)\n",
    "    valid_ppls.append(valid_ppl)\n",
    "\n",
    "    logger.info(f'Epoch {e+1}/{num_epochs}:')\n",
    "    logger.info(f'Train loss: {train_loss:.2f}, Perplexity: {train_ppl:.2f}')\n",
    "    logger.info(f'Valid loss: {valid_loss:.2f}, Perplexity: {valid_ppl:.2f}')\n",
    "\n",
    "    if valid_ppl < best_valid_ppl:\n",
    "        best_valid_ppl = valid_ppl\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'best_model.pt'))\n",
    "        logger.info(f'New best validation perplexity: {valid_ppl:.2f}')\n",
    "\n",
    "visualize(train_ppls, valid_ppls, f'Perplexity for {trial_name}', 'epochs', 'ppl', os.path.join(log_dir, 'perplexity.png'))\n",
    "visualize(train_losses, valid_losses, f'Loss for {trial_name}', 'epochs', 'loss', os.path.join(log_dir, 'losses.png'))\n",
    "\n",
    "# Load the best model for final evaluation\n",
    "model.load_state_dict(torch.load(os.path.join(log_dir, 'best_model.pt')))\n",
    "\n",
    "test_loss = evaluate(model, test_batches, criterion, device)\n",
    "test_ppl = calculate_perplexity(test_loss)\n",
    "logger.info(f\"Test Perplexity with best model: {test_ppl:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3b49074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed words: ['looking']\n",
      "Indices: [2, 676]\n",
      "output shape: torch.Size([1, 2, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0]\n",
      "output shape: torch.Size([1, 3, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0]\n",
      "output shape: torch.Size([1, 4, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0]\n",
      "output shape: torch.Size([1, 5, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 6, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 7, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 8, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 9, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 10, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 11, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 12, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 13, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 14, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 15, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 16, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 17, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 18, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "output shape: torch.Size([1, 19, 4899])\n",
      "next token : 0 predicted indices: [2, 676, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'looking <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, word_to_idx, idx_to_word, device, seed_text=\"looking\", max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f98bdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2281379\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_params = 0\n",
    "for param in list(model.parameters()):\n",
    "    n_params += param.numel()\n",
    "print(f\"Number of parameters: {n_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abec4976",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cudnn RNN backward can only be called in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# compute fisher influence function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fisher_influence_values = \u001b[43mget_fisher_influence_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mget_fisher_influence_function\u001b[39m\u001b[34m(model, train_batches, valid_batches, criterion, device)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_fisher_influence_function\u001b[39m(model, train_batches, valid_batches, criterion, device):\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     fisher_values = \u001b[43mget_fisher_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     valid_grads = get_impact_function(model, valid_batches, criterion, device)\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# valid_grads = torch.unsqueeze(valid_grads, 0)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mget_fisher_matrix\u001b[39m\u001b[34m(model, train_batches, criterion, device, epsilon, lambda_reg)\u001b[39m\n\u001b[32m     51\u001b[39m outputs, _ = model(inputs)\n\u001b[32m     52\u001b[39m loss = criterion(outputs.reshape(-\u001b[32m1\u001b[39m, outputs.size(-\u001b[32m1\u001b[39m)),\n\u001b[32m     53\u001b[39m                  targets.reshape(-\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# compute the empirical fisher matrix (diagonal of the real fisher matrix) \u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# and collect gradients\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model.named_parameters():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/if/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/if/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/if/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: cudnn RNN backward can only be called in training mode"
     ]
    }
   ],
   "source": [
    "# compute fisher influence function\n",
    "fisher_influence_values = get_fisher_influence_function(model, train_batches, valid_batches, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8742cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_values = {'fisher_influence_values': fisher_influence_values}\n",
    "save_file(influence_values, os.path.join(log_dir, 'fisher_influence_values.safetensors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f4ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "if",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
