{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170be645",
   "metadata": {},
   "source": [
    "# LSTM Training on Penn Treebank Dataset\n",
    "This notebook demonstrates how to train an LSTM model for language modeling using the Penn Treebank dataset in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae45624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hvaidya/miniconda3/envs/if/lib/python3.12/site-packages/torchtext/__init__.py:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\\n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \\n\"\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/home/hvaidya/miniconda3/envs/if/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch6detail10class_baseC2ERKSsS3_SsRKSt9type_infoS6_",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PennTreebank\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvocab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/if/lib/python3.12/site-packages/torchtext/__init__.py:18\u001b[39m\n\u001b[32m     15\u001b[39m     _WARN = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     20\u001b[39m _TEXT_BUCKET = \u001b[33m\"\u001b[39m\u001b[33mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m _CACHE_DIR = os.path.expanduser(os.path.join(_get_torch_home(), \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/if/lib/python3.12/site-packages/torchtext/_extension.py:64\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/if/lib/python3.12/site-packages/torchtext/_extension.py:58\u001b[39m, in \u001b[36m_init_extension\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils.is_module_available(\u001b[33m\"\u001b[39m\u001b[33mtorchtext._torchtext\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mtorchtext C++ Extension is not found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlibtorchtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/if/lib/python3.12/site-packages/torchtext/_extension.py:50\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m(lib)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/if/lib/python3.12/site-packages/torch/_ops.py:1392\u001b[39m, in \u001b[36m_Ops.load_library\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1387\u001b[39m path = _utils_internal.resolve_library_path(path)\n\u001b[32m   1388\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[32m   1389\u001b[39m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[32m   1390\u001b[39m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[32m   1391\u001b[39m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     \u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28mself\u001b[39m.loaded_libraries.add(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/if/lib/python3.12/ctypes/__init__.py:379\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28mself\u001b[39m._FuncPtr = _FuncPtr\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    381\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = handle\n",
      "\u001b[31mOSError\u001b[39m: /home/hvaidya/miniconda3/envs/if/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch6detail10class_baseC2ERKSsS3_SsRKSt9type_infoS6_"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "def read_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read().strip().split('\\n')\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Read the data\n",
    "train_path = 'data/ptb.train.txt'\n",
    "valid_path = 'data/ptb.valid.txt'\n",
    "\n",
    "train_data = read_data(train_path)\n",
    "valid_data = read_data(valid_path)\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(text_data, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for line in text_data:\n",
    "        counter.update(tokenize(line))\n",
    "    \n",
    "    # Filter words by frequency and add special tokens\n",
    "    words = ['<unk>', '<pad>', '<bos>', '<eos>'] + [word for word, count in counter.items() if count >= min_freq]\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "word_to_idx, idx_to_word = build_vocab(train_data)\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f'Vocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb1a8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LanguageModelingDataset.__init__() missing 2 required positional arguments: 'path' and 'text_field'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m         \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m tokenizer(text)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m vocab = build_vocab_from_iterator(yield_tokens(\u001b[43mPennTreebank\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m), specials=[\u001b[33m'\u001b[39m\u001b[33m<unk>\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      9\u001b[39m vocab.set_default_index(vocab[\u001b[33m'\u001b[39m\u001b[33m<unk>\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mTypeError\u001b[39m: LanguageModelingDataset.__init__() missing 2 required positional arguments: 'path' and 'text_field'"
     ]
    }
   ],
   "source": [
    "# Process data into tensors\n",
    "def process_data(data, word_to_idx):\n",
    "    processed = []\n",
    "    for line in data:\n",
    "        tokens = tokenize(line)\n",
    "        # Convert tokens to indices, replacing unknown words with <unk>\n",
    "        indices = [word_to_idx.get(token, word_to_idx['<unk>']) for token in tokens]\n",
    "        # Add <bos> and <eos> tokens\n",
    "        indices = [word_to_idx['<bos>']] + indices + [word_to_idx['<eos>']]\n",
    "        processed.append(torch.tensor(indices))\n",
    "    return processed\n",
    "\n",
    "train_tensors = process_data(train_data, word_to_idx)\n",
    "valid_tensors = process_data(valid_data, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4839ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches\n",
    "def create_batches(data_tensors, batch_size):\n",
    "    # Sort by length for efficient batching\n",
    "    data_tensors.sort(key=lambda x: len(x), reverse=True)\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, len(data_tensors), batch_size):\n",
    "        batch = data_tensors[i:i + batch_size]\n",
    "        # Pad sequences in the batch to the same length\n",
    "        max_len = len(batch[0])\n",
    "        padded = [torch.cat([seq, torch.tensor([word_to_idx['<pad>']] * (max_len - len(seq)))]) if len(seq) < max_len else seq for seq in batch]\n",
    "        # Stack into a single tensor\n",
    "        batches.append(torch.stack(padded))\n",
    "    return batches\n",
    "\n",
    "batch_size = 20\n",
    "train_batches = create_batches(train_tensors, batch_size)\n",
    "valid_batches = create_batches(valid_tensors, batch_size)\n",
    "\n",
    "print(f'Number of training batches: {len(train_batches)}')\n",
    "print(f'Number of validation batches: {len(valid_batches)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        batch_size = x.size(0)\n",
    "        if hidden is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "            hidden = (h0, c0)\n",
    "        \n",
    "        embeds = self.dropout(self.embed(x))  # (batch_size, seq_length, embed_size)\n",
    "        output, hidden = self.lstm(embeds, hidden)\n",
    "        output = self.dropout(output)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden\n",
    "\n",
    "# Initialize model\n",
    "embed_size = 200\n",
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "model = LSTMModel(vocab_size, embed_size, hidden_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<pad>'])\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede455ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_batches, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        # Input is all tokens except last, target is all tokens except first\n",
    "        inputs = batch[:, :-1]\n",
    "        targets = batch[:, 1:]\n",
    "        outputs, _ = model(inputs)\n",
    "        # Reshape for cross entropy\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_batches)\n",
    "\n",
    "# Generate text using the trained model\n",
    "def generate_text(model, start_words=['the'], max_length=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert start words to indices\n",
    "        current_ids = [word_to_idx.get(w, word_to_idx['<unk>']) for w in start_words]\n",
    "        current_ids = torch.tensor([current_ids])\n",
    "        hidden = None\n",
    "        generated_words = start_words.copy()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            output, hidden = model(current_ids, hidden)\n",
    "            # Get the most likely next word\n",
    "            next_word_idx = output[0, -1].argmax().item()\n",
    "            if idx_to_word[next_word_idx] == '<eos>':\n",
    "                break\n",
    "            generated_words.append(idx_to_word[next_word_idx])\n",
    "            current_ids = torch.tensor([[next_word_idx]])\n",
    "        \n",
    "        return ' '.join(generated_words)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_epoch(model, train_batches, criterion, optimizer)\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print('\\nGenerated text:')\n",
    "        print(generate_text(model, start_words=['the']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "if",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
