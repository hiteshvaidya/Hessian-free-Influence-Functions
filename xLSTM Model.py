# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s0EJjGaDcvWTLQiYuAsiETBOwLBi7mZh
"""

!pip install numpy==1.24.4 torch==2.1.0 torchtext==0.16.0 portalocker --force-reinstall --upgrade

import torch
import torch.nn as nn
from torchtext.datasets import PennTreebank
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import Dataset, DataLoader

tokenizer = get_tokenizer("basic_english")
def yield_tokens(data_iter):
    for line in data_iter:
        yield tokenizer(line)

vocab = build_vocab_from_iterator(yield_tokens(PennTreebank(split='train')), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])
tokens = [token for line in PennTreebank(split='train') for token in tokenizer(line)]
indexed_data = [vocab[token] for token in tokens]

def create_batches(data, seq_len=30, batch_size=32):
    total_len = (len(data) // (batch_size * seq_len)) * batch_size * seq_len
    data = torch.tensor(data[:total_len], dtype=torch.long)
    data = data.view(batch_size, -1)

    X, Y = [], []
    for i in range(0, data.size(1) - seq_len):
        x = data[:, i:i+seq_len]
        y = data[:, i+1:i+1+seq_len]
        X.append(x)
        Y.append(y)

    return torch.stack(X), torch.stack(Y)

class PTBDataset(Dataset):
    def __init__(self, data, seq_len=30):
        self.seq_len = seq_len
        self.samples = [
            (data[i:i+seq_len], data[i+1:i+1+seq_len])
            for i in range(len(data) - seq_len)
        ]

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        x, y = self.samples[idx]
        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)

train_dataset = PTBDataset(indexed_data, seq_len=30)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

class xLSTMCellPyTorch(nn.Module):
    def __init__(self, input_size, hidden_size, use_layernorm=False):
        super().__init__()
        self.use_layernorm = use_layernorm
        self.hidden_size = hidden_size
        self.W_x = nn.Linear(input_size, 4 * hidden_size)
        self.W_h = nn.Linear(hidden_size, 4 * hidden_size)
        self.layer_norm = nn.LayerNorm(4 * hidden_size) if use_layernorm else nn.Identity()

    def forward(self, x_t, h_prev, c_prev):
        gates = self.W_x(x_t) + self.W_h(h_prev)
        gates = self.layer_norm(gates)
        i, f, g, o = gates.chunk(4, dim=1)

        i = torch.sigmoid(i)
        f = torch.sigmoid(f)
        g = torch.tanh(g)
        o = torch.sigmoid(o)

        c_t = f * c_prev + i * g
        h_t = o * torch.tanh(c_t)
        return h_t, c_t

class xLSTMPyTorch(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, use_layernorm=False):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm_cell = xLSTMCellPyTorch(embed_size, hidden_size, use_layernorm)
        self.output_layer = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        batch_size, seq_len = x.size()
        h = torch.zeros(batch_size, self.lstm_cell.hidden_size, device=x.device)
        c = torch.zeros(batch_size, self.lstm_cell.hidden_size, device=x.device)
        outputs = []

        embedded = self.embedding(x)

        for t in range(seq_len):
            x_t = embedded[:, t, :]
            h, c = self.lstm_cell(x_t, h, c)
            logits = self.output_layer(h)
            outputs.append(logits.unsqueeze(1))

        return torch.cat(outputs, dim=1)  # [batch, seq_len, vocab_size]

def benchmark_pytorch_with_loader(train_loader, vocab_size, embed_size, hidden_size, epochs=10, lr=0.01):
    import time
    model = xLSTMPyTorch(vocab_size, embed_size, hidden_size, use_layernorm=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = torch.nn.CrossEntropyLoss()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    start_time = time.time()
    for epoch in range(epochs):
        total_loss = 0
        for x_batch, y_batch in train_loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            output = model(x_batch)
            loss = criterion(output.reshape(-1, vocab_size), y_batch.reshape(-1))
            optimizer.zero_grad()
            loss.backward(create_graph=True)  # Enables IF/FIM
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}")

    return model, time.time() - start_time

import math

def evaluate_model(model, data_loader, vocab_size):
    model.eval()
    device = next(model.parameters()).device
    criterion = nn.CrossEntropyLoss()
    total_loss = 0
    correct = 0
    total_tokens = 0

    with torch.no_grad():
        for x_batch, y_batch in data_loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            output = model(x_batch)  # [batch, seq_len, vocab_size]
            loss = criterion(output.reshape(-1, vocab_size), y_batch.reshape(-1))
            total_loss += loss.item()

            predictions = output.argmax(dim=2)
            correct += (predictions == y_batch).sum().item()
            total_tokens += y_batch.numel()

    avg_loss = total_loss / len(data_loader)
    accuracy = correct / total_tokens
    perplexity = math.exp(avg_loss)

    print(f"Evaluation - Loss: {avg_loss:.4f} | Accuracy: {accuracy:.4f} | Perplexity: {perplexity:.2f}")
    return avg_loss, accuracy, perplexity

vocab_size = len(vocab)
embed_size = 128
hidden_size = 256

model, training_time = benchmark_pytorch_with_loader(
    train_loader, vocab_size, embed_size, hidden_size, epochs=5, lr=0.01
)

evaluate_model(model, train_loader, vocab_size)
print(f"Training completed in {training_time:.2f} seconds")