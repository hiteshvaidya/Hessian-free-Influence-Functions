{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install loguru\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:19:44.066314Z","iopub.execute_input":"2025-05-02T21:19:44.066850Z","iopub.status.idle":"2025-05-02T21:19:48.209343Z","shell.execute_reply.started":"2025-05-02T21:19:44.066824Z","shell.execute_reply":"2025-05-02T21:19:48.208396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport nltk\nfrom nltk.corpus import treebank\nfrom collections import Counter\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom loguru import logger\nimport time\nimport argparse\nimport matplotlib.pyplot as plt\nimport os\nimport pickle as pkl\nfrom safetensors.torch import save_file, load_file\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:19:49.494262Z","iopub.execute_input":"2025-05-02T21:19:49.495138Z","iopub.status.idle":"2025-05-02T21:19:49.530912Z","shell.execute_reply.started":"2025-05-02T21:19:49.495109Z","shell.execute_reply":"2025-05-02T21:19:49.530392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set NLTK data path to project's data directory\nnltk_data_path = os.path.join(os.getcwd(), 'data', 'nltk_data')\nnltk.data.path.append(nltk_data_path)\n\n# Download required NLTK data\nif not os.path.exists(nltk_data_path):\n    print(f\"Downloading NLTK data to: {nltk_data_path}\")\n    nltk.download('treebank', download_dir=nltk_data_path)\n    nltk.download('punkt', download_dir=nltk_data_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:20:02.331480Z","iopub.execute_input":"2025-05-02T21:20:02.331835Z","iopub.status.idle":"2025-05-02T21:20:03.167135Z","shell.execute_reply.started":"2025-05-02T21:20:02.331812Z","shell.execute_reply":"2025-05-02T21:20:03.166565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_and_preprocess_data():\n    \"\"\"\n    Load and preprocess text data from Penn Treebank corpus.\n\n    Returns:\n        tuple: Contains three lists of tokenized sentences:\n            - train_data: Training set (80% of data)\n            - test_data: Test set (10% of data)\n            - val_data: Validation set (10% of data)\n\n    Each sentence is preprocessed by:\n        1. Converting to lowercase\n        2. Tokenizing using TreebankWordTokenizer\n        3. Splitting into train/test/val sets\n    \"\"\"\n    tokenizer = TreebankWordTokenizer()\n\n    # Get sentences from Penn Treebank corpus\n    sentences = treebank.sents()\n\n    # Process each sentence\n    processed = []\n    for sent in sentences:\n        # E sent is a list of words\n        # Join the sentence into a single string and tokenize\n        text = ' '.join(sent)\n        tokens = tokenizer.tokenize(text.lower())\n        processed.append(tokens)\n\n    # Split into train, test, and validation sets\n    train_data = processed[:int(len(processed) * 0.8)]\n    test_data = processed[int(len(processed) * 0.8):int(len(processed) * 0.9)]\n    val_data = processed[int(len(processed) * 0.9):]\n\n    return train_data, test_data, val_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:20:13.026953Z","iopub.execute_input":"2025-05-02T21:20:13.027215Z","iopub.status.idle":"2025-05-02T21:20:13.032313Z","shell.execute_reply.started":"2025-05-02T21:20:13.027194Z","shell.execute_reply":"2025-05-02T21:20:13.031663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_voab(data, min_freq=2):\n    \"\"\"\n    Build vocabulary from training data with minimum frequency threshold.\n\n    Args:\n        data (list): List of tokenized sentences where each sentence is a list of tokens\n        min_freq (int, optional): Minimum frequency threshold for including words. Defaults to 2.\n\n    Returns:\n        tuple: Contains two dictionaries:\n            - word_to_idx: Maps words to unique integer indices\n            - idx_to_word: Maps indices back to words\n\n    The vocabulary includes special tokens:\n        - <unk>: Unknown words\n        - <pad>: Padding token\n        - <bos>: Beginning of sentence\n        - <eos>: End of sentence\n    \"\"\"\n    counter = Counter()\n    for sent in data:\n        counter.update(sent)\n\n    # Create vocabulary with special tokens\n    words = ['<unk>', '<pad>', '<bos>', '<eos>']\n    words.extend([word for word, freq in counter.items() if freq >= min_freq])\n\n    word_to_idx = {word: idx for idx, word in enumerate(words)}\n    idx_to_word = {idx: word for idx, word in enumerate(words)}\n\n    return word_to_idx, idx_to_word\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:20:20.727324Z","iopub.execute_input":"2025-05-02T21:20:20.727588Z","iopub.status.idle":"2025-05-02T21:20:20.732783Z","shell.execute_reply.started":"2025-05-02T21:20:20.727568Z","shell.execute_reply":"2025-05-02T21:20:20.732112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_data(data, word_to_idx):\n    \"\"\"\n    Process raw text data into model-ready format by converting tokens to indices.\n\n    Args:\n        data (list): List of tokenized sentences where each sentence is a list of tokens\n        word_to_idx (dict): Dictionary mapping words to unique integer indices\n\n    Returns:\n        list: List of torch tensors, where each tensor contains the indices for a sentence\n            including <bos> and <eos> tokens\n\n    Each sentence is processed by:\n        1. Converting tokens to their vocabulary indices\n        2. Adding beginning-of-sentence (<bos>) and end-of-sentence (<eos>) tokens\n        3. Converting to a PyTorch tensor\n    \"\"\"\n    processed = []\n    for sent in data:\n        # convert tokens to indices\n        indices = [word_to_idx.get(token, word_to_idx['<unk>']) for token in sent]\n        # Add <bos> and <eos> tokens\n        indices = [word_to_idx['<bos>']] + indices + [word_to_idx['<eos>']]\n        processed.append(torch.tensor(indices))\n\n    return processed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:20:28.061958Z","iopub.execute_input":"2025-05-02T21:20:28.062787Z","iopub.status.idle":"2025-05-02T21:20:28.067607Z","shell.execute_reply.started":"2025-05-02T21:20:28.062753Z","shell.execute_reply":"2025-05-02T21:20:28.066923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_batches(data, word_to_idx, batch_size=32):\n    \"\"\"\n    Create batches from processed data for model training.\n\n    Args:\n        data (list): List of torch tensors containing processed sentences\n        word_to_idx (dict): Dictionary mapping words to unique integer indices\n        batch_size (int, optional): Size of each batch. Defaults to 32.\n\n    Returns:\n        list: List of torch tensors, where each tensor is a batch of padded sequences\n            with shape (batch_size, max_sequence_length)\n\n    The function:\n        1. Sorts sequences by length in descending order for efficient padding\n        2. Groups sequences into batches of specified size\n        3. Pads shorter sequences in each batch to match the longest sequence\n        4. Converts batches to torch tensors\n    \"\"\"\n    data.sort(key=lambda x: len(x), reverse=True)\n    total_len = len(data)\n    num_batches = (total_len + batch_size - 1) // batch_size\n\n    batches = []\n    for i in range(0, len(data), batch_size):\n        batch = data[i:i+batch_size]\n        max_len = len(batch[0])\n        padded = [torch.cat([seq, torch.tensor([word_to_idx['<pad>']] * (max_len - len(seq)))]) if len(seq) < max_len else seq for seq in batch]\n        batches.append(torch.stack(padded))\n    return batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:20:35.386393Z","iopub.execute_input":"2025-05-02T21:20:35.386847Z","iopub.status.idle":"2025-05-02T21:20:35.392322Z","shell.execute_reply.started":"2025-05-02T21:20:35.386823Z","shell.execute_reply":"2025-05-02T21:20:35.391674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Network(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, cell='lstm', dropout=0.5):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, \n                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, x, hidden=None):\n        # Ensure input tensor is on correct device\n        x = x.to(next(self.parameters()).device)\n        \n        embeds = self.embed(x)  # (batch, seq_len, embed_size)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        logits = self.fc(lstm_out)  # (batch, seq_len, vocab_size)\n        return logits, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:32:37.495224Z","iopub.execute_input":"2025-05-02T21:32:37.495543Z","iopub.status.idle":"2025-05-02T21:32:37.501523Z","shell.execute_reply.started":"2025-05-02T21:32:37.495522Z","shell.execute_reply":"2025-05-02T21:32:37.500546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model, train_batches, criterion, optimizer, device):\n    model.train()\n    total_loss = 0.0\n    \n    for batch in train_batches:\n        optimizer.zero_grad()\n        \n        # Move batch to device\n        batch = batch.to(device)\n        \n        # Inputs and targets\n        inputs = batch[:, :-1]  # (batch, seq_len-1)\n        targets = batch[:, 1:]   # (batch, seq_len-1)\n        \n        # Forward pass\n        outputs, _ = model(inputs)  # (batch, seq_len-1, vocab_size)\n        \n        # Reshape for loss calculation\n        loss = criterion(outputs.reshape(-1, outputs.size(-1)), \n                       targets.reshape(-1))\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(train_batches)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:32:38.516690Z","iopub.execute_input":"2025-05-02T21:32:38.516953Z","iopub.status.idle":"2025-05-02T21:32:38.522434Z","shell.execute_reply.started":"2025-05-02T21:32:38.516934Z","shell.execute_reply":"2025-05-02T21:32:38.521692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to load vocabulary mappings\ndef load_vocab(vocab_path):\n    with open(vocab_path, 'r') as f:\n        vocab_dict = json.load(f)\n    return vocab_dict['word_to_idx'], vocab_dict['idx_to_word']\n\n# function to load tensors from safetensors file\ndef load_tensors(file_path):\n    loaded_tensors = load_file(file_path)\n    return loaded_tensors['train'], loaded_tensors['valid'], loaded_tensors['test']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:32:50.201429Z","iopub.execute_input":"2025-05-02T21:32:50.201684Z","iopub.status.idle":"2025-05-02T21:32:50.206284Z","shell.execute_reply.started":"2025-05-02T21:32:50.201668Z","shell.execute_reply":"2025-05-02T21:32:50.205433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text(model, word_to_idx, idx_to_word, device, seed_text='the', max_length=20):\n    \"\"\"Generate text using the trained language model.\n\n    Args:\n        model: The trained language model\n        word_to_idx (dict): Dictionary mapping words to indices\n        idx_to_word (dict): Dictionary mapping indices to words\n        seed_text (str, optional): Initial text to condition generation on. Defaults to 'the'.\n        max_length (int, optional): Maximum number of words to generate. Defaults to 20.\n\n    Returns:\n        str: Generated text as a space-separated string of words\n    \"\"\"\n    model.eval()\n    words = seed_text.lower().split()\n    indices = [word_to_idx.get(word, word_to_idx['<unk>']) for word in words]\n    indices = [word_to_idx['<bos>']] + indices\n\n    with torch.no_grad():\n        for _ in range(max_length):\n            input_tensor = torch.tensor(indices).unsqueeze(0).to(device)\n            output, _ = model(input_tensor)\n            next_token_idx = output[0, -1].argmax().item()\n\n            if next_token_idx == word_to_idx['<eos>']:\n                break\n\n            indices.append(next_token_idx)\n\n    generated_words = [idx_to_word[idx] for idx in indices[1:]] # skip <bos>\n\n    return ' '.join(generated_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:32:53.779221Z","iopub.execute_input":"2025-05-02T21:32:53.779596Z","iopub.status.idle":"2025-05-02T21:32:53.786421Z","shell.execute_reply.started":"2025-05-02T21:32:53.779569Z","shell.execute_reply":"2025-05-02T21:32:53.785680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_validation_gradients(model, valid_batches, criterion, device):\n    \"\"\"Compute accumulated validation gradients\"\"\"\n    model.train()  # Switch to train mode for gradient computation\n    valid_grads = {name: torch.zeros_like(param.data)\n                  for name, param in model.named_parameters()\n                  if param.requires_grad}\n\n    # Enable gradient computation context\n    with torch.set_grad_enabled(True):\n        for batch in valid_batches:\n            model.zero_grad()\n            inputs = batch[:, :-1].to(device)\n            targets = batch[:, 1:].to(device)\n\n            outputs, _ = model(inputs)\n            loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n                            targets.reshape(-1))\n\n            # Compute gradients\n            loss.backward(retain_graph=True)\n\n            # Accumulate gradients\n            for name, param in model.named_parameters():\n                if param.grad is not None:\n                    valid_grads[name] += param.grad.detach().clone()\n\n    model.eval()  # Return to eval mode\n    return torch.cat([g.view(-1) for g in valid_grads.values()])\n\ndef compute_influences(model, train_batches, valid_grad, criterion, device):\n    \"\"\"Compute influence values for each training example\"\"\"\n    influences = []\n    model.train()  # Ensure training mode for gradients\n\n    for batch in train_batches:\n        for b in range(batch.size(0)):\n            model.zero_grad()\n            inputs = batch[b:b+1, :-1].to(device)\n            targets = batch[b:b+1, 1:].to(device)\n\n            # Forward pass with gradient tracking\n            with torch.set_grad_enabled(True):\n                outputs, _ = model(inputs)\n                loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n                                targets.reshape(-1))\n\n            # Backward pass\n            loss.backward(retain_graph=True)\n\n            # Get and clone gradients\n            train_grad = torch.cat([\n                param.grad.detach().clone().view(-1)\n                for param in model.parameters()\n                if param.requires_grad and param.grad is not None\n            ]).to(device)\n\n            # Calculate influence\n            influence = torch.dot(valid_grad.to(device), train_grad)\n            influences.append(influence.item())\n\n    model.eval()\n    return torch.tensor(influences, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:32:56.489481Z","iopub.execute_input":"2025-05-02T21:32:56.489997Z","iopub.status.idle":"2025-05-02T21:32:56.498899Z","shell.execute_reply.started":"2025-05-02T21:32:56.489975Z","shell.execute_reply":"2025-05-02T21:32:56.498044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\ndef visualize_results(initial_train, initial_val, retrain_train, retrain_val):\n    \"\"\"Plot training/validation metrics before and after influence selection\"\"\"\n    plt.figure(figsize=(15, 5))\n\n    # Loss plot\n    plt.subplot(1, 2, 1)\n    plt.plot(initial_train, '-o', label='Initial Train Loss')\n    plt.plot(initial_val, '-o', label='Initial Val Loss')\n    plt.plot(retrain_train, '--x', label='Retrain Train Loss')\n    plt.plot(retrain_val, '--x', label='Retrain Val Loss')\n    plt.title(\"Loss Comparison\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n\n    # Perplexity plot\n    plt.subplot(1, 2, 2)\n    plt.plot(torch.exp(torch.tensor(initial_train)), '-o', label='Initial Train PPL')\n    plt.plot(torch.exp(torch.tensor(initial_val)), '-o', label='Initial Val PPL')\n    plt.plot(torch.exp(torch.tensor(retrain_train)), '--x', label='Retrain Train PPL')\n    plt.plot(torch.exp(torch.tensor(retrain_val)), '--x', label='Retrain Val PPL')\n    plt.title(\"Perplexity Comparison\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T21:33:02.270606Z","iopub.execute_input":"2025-05-02T21:33:02.271302Z","iopub.status.idle":"2025-05-02T21:33:02.277209Z","shell.execute_reply.started":"2025-05-02T21:33:02.271279Z","shell.execute_reply":"2025-05-02T21:33:02.276419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#new\n# Initialize metric trackers\ninitial_train_losses = []\ninitial_valid_losses = []\nretrain_train_losses = []\nretrain_valid_losses = []\ndef main():\n\n    # Initial setup\n    trial_name = 'influence_baseline'\n    device = torch.device('cuda' if torch.cuda.is_available() else 'mps'\n                        if torch.backends.mps.is_available() else 'cpu')\n\n    # Load data and build vocabulary\n    train_data, valid_data, test_data = load_and_preprocess_data()\n    word_to_idx, idx_to_word = build_voab(train_data)\n\n    # Process data and create batches\n    train_tensors = process_data(train_data, word_to_idx)\n    valid_tensors = process_data(valid_data, word_to_idx)\n    test_tensors = process_data(test_data, word_to_idx)\n\n    batch_size = 32\n    train_batches = create_batches(train_tensors, word_to_idx, batch_size)\n    valid_batches = create_batches(valid_tensors, word_to_idx, batch_size)\n    test_batches = create_batches(test_tensors, word_to_idx, batch_size)\n\n    # Initialize model\n    model = Network(\n        vocab_size=len(word_to_idx),\n        embed_size=300,\n        hidden_size=512,\n        num_layers=1\n    ).to(device)\n      # Verify parameter initialization\n    print(\"Model parameters device:\", next(model.parameters()).device)\n    criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<pad>'])\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Phase 1: Initial Training\n    logger.info(\"Initial Training Phase\")\n    train_losses, valid_losses = [], []\n    for epoch in range(25):\n        train_loss = train(model, train_batches, criterion, optimizer, device)\n        valid_loss = evaluate(model, valid_batches, criterion, device)\n\n        logger.info(f\"Epoch {epoch+1}/25\")\n        logger.info(f\"Train Loss: {train_loss:.2f} | Valid Loss: {valid_loss:.2f}\")\n        logger.info(f\"Train PPL: {calculate_perplexity(train_loss):.2f} | Valid PPL: {calculate_perplexity(valid_loss):.2f}\")\n        initial_train_losses.append(train_loss)\n        initial_valid_losses.append(valid_loss)\n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n\n    # Phase 2: Compute Influences\n    logger.info(\"Computing Influence Values\")\n\n    # Ensure gradient computation context\n    with torch.autograd.set_detect_anomaly(True):\n        valid_grad = get_validation_gradients(model, valid_batches, criterion, device)\n        influences = compute_influences(model, train_batches, valid_grad, criterion, device)\n\n    # Phase 3: Select Top 30%\n    logger.info(\"Selecting Top 30% Examples\")\n    selected_data = select_top_examples(train_data, influences, 0.3)\n    selected_tensors = process_data(selected_data, word_to_idx)\n    selected_batches = create_batches(selected_tensors, word_to_idx, batch_size)\n\n    # Phase 4: Retrain on Selected Data\n    logger.info(\"Retraining on Selected Examples\")\n    model = Network(  # Reinitialize model\n        vocab_size=len(word_to_idx),\n        embed_size=300,\n        hidden_size=512,\n        num_layers=1\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n       # Verify first batch data\n    sample_batch = next(iter(train_batches))\n    print(\"Sample batch shape:\", sample_batch.shape)\n    print(\"Sample inputs:\", sample_batch[0, :-1])\n    print(\"Sample targets:\", sample_batch[0, 1:])\n    retrain_losses = []\n    for epoch in range(25):\n        train_loss = train(model, selected_batches, criterion, optimizer, device)\n        valid_loss = evaluate(model, valid_batches, criterion, device)\n\n        logger.info(f\"Retrain Epoch {epoch+1}/25\")\n        logger.info(f\"Train Loss: {train_loss:.2f} | Valid Loss: {valid_loss:.2f}\")\n        logger.info(f\"Train PPL: {calculate_perplexity(train_loss):.2f} | Valid PPL: {calculate_perplexity(valid_loss):.2f}\")\n        retrain_train_losses.append(train_loss)\n        retrain_valid_losses.append(valid_loss)\n        retrain_losses.append(train_loss)\n\n    # Final Evaluation\n    visualize_results(\n    initial_train_losses,\n    initial_valid_losses,\n    retrain_train_losses,\n    retrain_valid_losses\n)\n    test_loss = evaluate(model, test_batches, criterion, device)\n    logger.info(f\"Final Test PPL: {calculate_perplexity(test_loss):.2f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T22:14:16.334698Z","iopub.execute_input":"2025-05-02T22:14:16.335243Z","execution_failed":"2025-05-02T22:14:23.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_text(model, word_to_idx, idx_to_word, device, seed_text=\"the\", max_length=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T22:03:30.523730Z","iopub.execute_input":"2025-05-02T22:03:30.524025Z","iopub.status.idle":"2025-05-02T22:03:30.538283Z","shell.execute_reply.started":"2025-05-02T22:03:30.524004Z","shell.execute_reply":"2025-05-02T22:03:30.537233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compare_generations(model1, model2, word_to_idx, idx_to_word, device, \n                       seed_texts=[\"the\", \"company\", \"market\"], max_length=20):\n    \"\"\"Compare text generation between two models\"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(\"Text Generation Comparison\")\n    print(\"=\"*50)\n    \n    for seed in seed_texts:\n        # Generate from both models\n        gen1 = generate_text(model1, word_to_idx, idx_to_word, device, seed, max_length)\n        gen2 = generate_text(model2, word_to_idx, idx_to_word, device, seed, max_length)\n        \n        # Display results\n        print(f\"\\nSeed: '{seed}'\")\n        print(f\"Initial Model:  {gen1}\")\n        print(f\"Retrained Model: {gen2}\")\n        print(\"-\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T22:12:12.234066Z","iopub.execute_input":"2025-05-02T22:12:12.234792Z","iopub.status.idle":"2025-05-02T22:12:12.239717Z","shell.execute_reply.started":"2025-05-02T22:12:12.234768Z","shell.execute_reply":"2025-05-02T22:12:12.238846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}