# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s0EJjGaDcvWTLQiYuAsiETBOwLBi7mZh
"""

# !pip install numpy==1.24.4 torch==2.1.0 torchtext==0.16.0 portalocker --force-reinstall --upgrade

# !pip install tqdm

import torch
import torch.nn as nn
from torchtext.datasets import PennTreebank
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import Dataset, DataLoader
from safetensors.torch import save_file, load_file

tokenizer = get_tokenizer("basic_english")
def yield_tokens(data_iter):
    for line in data_iter:
        yield tokenizer(line)

vocab = build_vocab_from_iterator(yield_tokens(PennTreebank(split='train')), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])
tokens = [token for line in PennTreebank(split='train') for token in tokenizer(line)]
indexed_data = [vocab[token] for token in tokens]

def create_batches(data, seq_len=30, batch_size=32):
    total_len = (len(data) // (batch_size * seq_len)) * batch_size * seq_len
    data = torch.tensor(data[:total_len], dtype=torch.long)
    data = data.view(batch_size, -1)

    X, Y = [], []
    for i in range(0, data.size(1) - seq_len):
        x = data[:, i:i+seq_len]
        y = data[:, i+1:i+1+seq_len]
        X.append(x)
        Y.append(y)

    return torch.stack(X), torch.stack(Y)

class PTBDataset(Dataset):
    def __init__(self, data, seq_len=30):
        self.seq_len = seq_len
        self.samples = [
            (data[i:i+seq_len], data[i+1:i+1+seq_len])
            for i in range(len(data) - seq_len)
        ]

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        x, y = self.samples[idx]
        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)

train_dataset = PTBDataset(indexed_data, seq_len=30)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

tokens = [token for line in PennTreebank(split='valid') for token in tokenizer(line)]
valid_indexed_data = [vocab[token] for token in tokens]
valid_dataset = PTBDataset(valid_indexed_data, seq_len=30)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

class xLSTMCellPyTorch(nn.Module):
    def __init__(self, input_size, hidden_size, use_layernorm=False):
        super().__init__()
        self.use_layernorm = use_layernorm
        self.hidden_size = hidden_size
        self.W_x = nn.Linear(input_size, 4 * hidden_size)
        self.W_h = nn.Linear(hidden_size, 4 * hidden_size)
        self.layer_norm = nn.LayerNorm(4 * hidden_size) if use_layernorm else nn.Identity()

    def forward(self, x_t, h_prev, c_prev):
        gates = self.W_x(x_t) + self.W_h(h_prev)
        gates = self.layer_norm(gates)
        i, f, g, o = gates.chunk(4, dim=1)

        i = torch.sigmoid(i)
        f = torch.sigmoid(f)
        g = torch.tanh(g)
        o = torch.sigmoid(o)

        c_t = f * c_prev + i * g
        h_t = o * torch.tanh(c_t)
        return h_t, c_t

class xLSTMPyTorch(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, use_layernorm=False):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm_cell = xLSTMCellPyTorch(embed_size, hidden_size, use_layernorm)
        self.output_layer = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        batch_size, seq_len = x.size()
        h = torch.zeros(batch_size, self.lstm_cell.hidden_size, device=x.device)
        c = torch.zeros(batch_size, self.lstm_cell.hidden_size, device=x.device)
        outputs = []

        embedded = self.embedding(x)

        for t in range(seq_len):
            x_t = embedded[:, t, :]
            h, c = self.lstm_cell(x_t, h, c)
            logits = self.output_layer(h)
            outputs.append(logits.unsqueeze(1))

        return torch.cat(outputs, dim=1)  # [batch, seq_len, vocab_size]

from tqdm import tqdm
def benchmark_pytorch_with_loader(train_loader, vocab_size, embed_size, hidden_size, epochs=10, lr=0.01):
    import time
    model = xLSTMPyTorch(vocab_size, embed_size, hidden_size, use_layernorm=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = torch.nn.CrossEntropyLoss()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    start_time = time.time()
    best_loss = float('inf')
    for epoch in range(epochs):
        total_loss = 0
        tqdm.write(f"Training Epoch {epoch}")
        for x_batch, y_batch in tqdm(train_loader):
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            output = model(x_batch)
            loss = criterion(output.reshape(-1, vocab_size), y_batch.reshape(-1))
            optimizer.zero_grad()
            loss.backward(create_graph=True)  # Enables IF/FIM
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1} | Avg Loss: {avg_loss:.4f} | Perplexity: {math.exp(avg_loss):.4f}")
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), "best_model.pth")

    return model, time.time() - start_time

import math

def evaluate_model(model, data_loader, vocab_size):
    model.eval()
    device = next(model.parameters()).device
    criterion = nn.CrossEntropyLoss()
    total_loss = 0
    correct = 0
    total_tokens = 0

    with torch.no_grad():
        for x_batch, y_batch in data_loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)

            output = model(x_batch)  # [batch, seq_len, vocab_size]
            loss = criterion(output.reshape(-1, vocab_size), y_batch.reshape(-1))
            total_loss += loss.item()

            predictions = output.argmax(dim=2)
            correct += (predictions == y_batch).sum().item()
            total_tokens += y_batch.numel()

    avg_loss = total_loss / len(data_loader)
    accuracy = correct / total_tokens
    perplexity = math.exp(avg_loss)

    print(f"Evaluation - Loss: {avg_loss:.4f} | Accuracy: {accuracy:.4f} | Perplexity: {perplexity:.2f}")
    return avg_loss, accuracy, perplexity

vocab_size = len(vocab)
embed_size = 128
hidden_size = 256

print(f"benchmarking pytorch with loader")
# model, training_time = benchmark_pytorch_with_loader(
#     train_loader, vocab_size, embed_size, hidden_size, epochs=25, lr=0.01
# )

# evaluate_model(model, train_loader, vocab_size)
# print(f"Training completed in {training_time:.2f} seconds")

def generate_text(model, vocab, inverse_vocab, device, seed_text='the', max_length=20):
    """Generate text using the trained language model."""
    model.eval()
    words = seed_text.lower().split()
    print(f"seed words: {words}")
    indices = [vocab[word] for word in words]  # Fixed line

    with torch.no_grad():
        while len(indices) < max_length:
            input_tensor = torch.tensor(indices).unsqueeze(0).to(device)
            output = model(input_tensor)  # Only one output (logits)
            next_token_idx = output[0, -1].argmax().item()

            if next_token_idx == vocab['<eos>']:
                break

            indices.append(next_token_idx)

    generated_words = [inverse_vocab[idx] for idx in indices[1:]]  # skip <bos>
    return ' '.join(generated_words)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = xLSTMPyTorch(vocab_size, embed_size, hidden_size, use_layernorm=True)
state_dict = torch.load("best_model.pth")
model.load_state_dict(state_dict)
model.to(device)

# Fix for inverse_vocab
inverse_vocab = {idx: token for idx, token in enumerate(vocab.get_itos())}

print("Generating text:")
print(generate_text(model, vocab, inverse_vocab, device, seed_text='expectations that', max_length=20))


# def get_impact_function(model, valid_batches, criterion, device):
#     """Get the impact function for each parameter in the model.
    
#     The impact function is calculated by accumulating gradients over the validation set.
#     This represents how much each parameter impacts the model's performance on validation data.
    
#     Args:
#         model: The trained language model
#         valid_batches: Validation data batches
#         criterion: Loss function
#         device: Device to run computations on (cuda/cpu/mps)
        
#     Returns:
#         dict: Dictionary mapping parameter names to their accumulated gradients
#     """
#     # initialize gradient dictionary to store gradients of each parameter
#     valid_grads = {name: torch.zeros_like(param.data) for name, param in model.named_parameters() if param.requires_grad}
    
#     # set model to training mode so the dropout is applied
#     model.train()

#     for batch in valid_batches:
#         model.zero_grad()
#         inputs = batch[:, :-1].to(device)
#         targets = batch[:, 1:].to(device)
        
#         outputs, _ = model(inputs)
#         loss = criterion(outputs.reshape(-1, outputs.size(-1)),
#                          targets.reshape(-1))
#         loss.backward()

#         for name, param in model.named_parameters():
#             if param.grad is not None:
#                 valid_grads[name] += param.grad.detach()
#     model.eval()
#     grads = torch.cat([value.view(-1) for name, value in valid_grads.items()])
    
#     return grads
    
# def get_fisher_matrix(model, train_loader, criterion, device, epsilon=1e-7, lambda_reg=1E-3):
#     model.train()
#     fisher_matrix = {name: torch.zeros_like(param.data) for name, param in model.named_parameters() if param.requires_grad}
#     grad_matrix = {name: torch.zeros_like(param.data) for name, param in model.named_parameters() if param.requires_grad}
#     n_samples = 0
    
#     for batchX, batchY in train_batches:
#         model.zero_grad()
#         batchX, batchY = batchX.to(device), batchY.to(device)
#         n_samples += batchX.size(0)
#         outputs = model(batchX)
#         loss = criterion(outputs.reshape(-1, vocab_size), batchY.reshape(-1))
#         model.zero_grad()
#         loss.backward(create_graph=True)  # Enables IF/FIM

#         # compute the empirical fisher matrix (diagonal of the real fisher matrix) 
#         # and collect gradients
#         for name, param in model.named_parameters():
#             if param.grad is not None:
#                 fisher_matrix[name] += torch.pow(param.grad.detach(), 2)
        
#     # normalize the fisher values per parameter and invert
#     for name in fisher_matrix:
#         temp = (fisher_matrix[name] / (n_samples)) + lambda_reg * 1.0
#         fisher_matrix[name] = 1 / (temp + epsilon)

#     grads = torch.cat([value.view(-1) for name, value in fisher_matrix.items()])
#     model.eval()
#     return grads


# def get_fisher_influence_function(model, train_loader, valid_loader, criterion, device):
#     fisher_values = get_fisher_matrix(model, train_loader, criterion, device)
#     valid_grads = get_impact_function(model, valid_batches, criterion, device)
#     # valid_grads = torch.unsqueeze(valid_grads, 0)
    
#     influence_values = []
#     model.train()
#     for batch in train_batches:
#         for b in range(batch.size(0)):
#             model.zero_grad()
#             inputs = batch[b: b+1, :-1].to(device)
#             targets = batch[b: b+1, 1:].to(device)
#             outputs, _ = model(inputs)
#             loss = criterion(outputs.reshape(-1, outputs.size(-1)),
#                          targets.reshape(-1))
#             loss.backward()
            
#             grads = torch.cat([param.grad.detach().view(-1) for name, param in model.named_parameters() if param.requires_grad])

#             # fisher_values = fisher_values * grads

#             influence_values.append(torch.sum(valid_grads * fisher_values * grads))
#     model.eval()
#     influence_values = torch.tensor(influence_values)
#     print(f"influence_values: {influence_values.shape}")
#     return influence_values

# import os
# fisher_influence_values = get_fisher_influence_function(model, train_loader, valid_loader, criterion=nn.CrossEntropyLoss(), device=device)
# influence_values = {'fisher_influence_values': fisher_influence_values}
# save_file(influence_values, os.path.join('./', 'fisher_influence_values.safetensors'))
